{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction and blind source separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires **HyperSpy v2.0** or above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to perform matrix factorization and blind source separation on spectra using HyperSpy. The same procedure can be used to analyse objects of higher dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits and changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 17/06/2010 Created by Francisco de la Peña for the EELSLab workshop at the LPS, Universté Paris-Sud\n",
    "* 23/8/2016 Michael Walls. Extra explanations.\n",
    "* 27/7/2016 Francisco de la Peña. Updated for HyperSpy v1.0.1.\n",
    "* 6/3/2016 Francisco de la Peña. Adapted from previous tutorials for the SCANDEM workshop.\n",
    "* 22/08/2024 Francisco de la Peña. Shorten the tutorials by starting directly with the dataset displaying energy instability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Singular value decomposition](#1.-Singular-value-decomposition)\n",
    "2. [Blind source separation](#2.-Blind-source-separation)\n",
    "3. [Pre-processing](#3.-Pre-processing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a powerful method for reducing noise in hyperspectral datasets. Here we begin with a few lines of matrix algebra outlining the principles of the technique, but the demo can be followed easily without a full understanding of the mathematics. As with the \"getting started\" demo you can run this one interactively by downloading it and saving it as a .ipynb file.\n",
    "\n",
    "Lets start by supposing a line-spectrum $D$ that can be described by a\n",
    "linear model.\n",
    "\n",
    "$D=\\left[d_{i,j}\\right]_{m\\times n}={\\displaystyle \\left[{\\color{green}p_{i,j}}\\right]_{m\\times{\\color{red}l}}\\times}\\left[{\\color{red}{\\color{blue}s_{i,j}}}\\right]_{{\\color{red}l}\\times n}$\n",
    "where $m$ is the *number of pixels* in the line scan, $n$ the *number of channels* in the spectrum and $l$ the *number of components* e.g. spectra of individual compounds.\n",
    "\n",
    "Normally, what we actually measure is a noisy version of $D$, $D'$,\n",
    "\n",
    "$D'={\\displaystyle \\left[d'_{i,j}\\right]_{m\\times n}=\\left[{\\color{green}p_{i,j}}\\right]_{m\\times{\\color{red}l}}\\times}\\left[{\\color{red}{\\color{blue}s_{i,j}}}\\right]_{{\\color{red}l}\\times n}+\\mathrm{Noise}$\n",
    "\n",
    "\n",
    "$D'$ could be factorized as follows:\n",
    "\n",
    "$D'={\\displaystyle \\left[{\\tilde{\\color{green}p}}_{{i,j}}\\right]_{m\\times{\\color{red}k}}\\times}\\left[\\tilde{s}_{i,j}\\right]_{{\\color{red}k}\\times n}\n",
    "$ where $k\\leq\\min\\left(m,n\\right)$.\n",
    "\n",
    "Extra constraints are needed to fully determine the matrix factorization. When we add the orthogonality constraint we refer to this decomposition as singular value decomposition (SVD).\n",
    "\n",
    "In our assumption of a linear model:\n",
    "\n",
    "$D'={\\displaystyle \\left[{\\tilde{\\color{green}p}}_{{i,j}}\\right]_{m\\times{\\color{red}l}}\\times}\\left[\\tilde{s}_{i,j}\\right]_{{\\color{red}l}\\times n}+\n",
    "{\\displaystyle \\left[{\\tilde{\\color{green}p}}_{{i,j}}\\right]_{m\\times{\\color{red}{k-l}}}\\times}\\left[\\tilde{s}_{i,j}\\right]_{{\\color{red}{k-l}}\\times n}$\n",
    "\n",
    "With \n",
    "\n",
    "$D\\approx{\\displaystyle \\left[{\\tilde{\\color{green}p}}_{{i,j}}\\right]_{m\\times{\\color{red}l}}\\times}\\left[\\tilde{s}_{i,j}\\right]_{{\\color{red}l}\\times n}$\n",
    "\n",
    "$\\mathrm{Noise}\\approx{\\displaystyle \\left[{\\tilde{\\color{green}p}}_{{i,j}}\\right]_{m\\times{\\color{red}{k-l}}}\\times}\\left[\\tilde{s}_{i,j}\\right]_{{\\color{red}{k-l}}\\times n}$\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading the data for this demo, activating the matplotlib backend, importing HyperSpy and loading a dataset.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up matplotlib and import Hyperspy\n",
    "\n",
    "**NOTE**: In the online version of this document we use the `inline` backend that displays interactive figures inside the Jupyter Notebook. However, for interactive data analysis purposes most would prefer to use the `qt4`, `wx` or `nbagg` backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# or qt4 etc\n",
    "import hyperspy.api as hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = hs.load(\"datasets/CL.hspy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a synthetic electron energy-loss spectroscopy dataset. The procedure, although not fully general, can easily be used as is or with minor adaptation to analyse other kinds of data including images and higher dimensional signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.preferences.gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform SVD in HyperSpy we use the `decomposition` method that, by default, performs SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.decomposition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the decomposition is stored in the `learning_results` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.learning_results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD decomposes the data in so-called \"components\" and sorts them in order of decreasing relevance. It is often useful to estimate the dimensionality of the data by plotting the explained variance against the component index on a logarithmic y-scale. This plot is sometimes called a scree-plot and it should drop quickly, eventually becoming a slowly descending line. The point at which it becomes linear is often a good estimation of the dimensionality of the data (or equivalently, the number of components that should be retained).\n",
    "\n",
    "To plot the scree plot, run the `plot_explained_variance_ratio` method e.g.:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.plot_explained_variance_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the scree plot appears to overestimate the number of relevant components this time. By examining the dataset, we can see that the data suffers from energy instability, which is evident from the shift in the EELS features. To address this issue, we can use a simultaneously acquired low-loss spectrum to align the core-loss spectrum image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = hs.load(\"datasets/LL.hspy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll.align_zero_loss_peak(also_align=[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.decomposition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.plot_explained_variance_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time from the scree plot we estimate that there are 5 principal components. However, we know (because this is obviously a synthetic dataset), that the correct number of components is 4.\n",
    "\n",
    "We can store the scree plot as a `Spectrum` instance using the following method before we try to improve the decomposition: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scree_plot = s.get_explained_variance_ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scree_plot.isig[:30].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shot noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA assumes gaussian noise, however, the noise in EELS spectra is approximately poissonian (shot noise). It is possible to approximately \"normalise\" the noise by using a liner transformation, which should result in a better decomposition of data with shot noise. This is done in HyperSpy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.decomposition(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the scree plot of this and the previous decomposition in the same figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.plot_explained_variance_ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hs.plot.plot_spectra([scree_plot.isig[:20],\n",
    "                          s.get_explained_variance_ratio().isig[:20]],\n",
    "                          legend=(\"Std\", \"Normalized\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's improve the plot using some [matplotlib](http://matplotlib.org/) commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_yscale('log')\n",
    "ax.lines[0].set_marker(\"o\")\n",
    "ax.lines[1].set_marker(\"o\")\n",
    "ax.figure #for details of how these commands work see the matplotlib webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the explained variance of the first four components in the normalized decomposition is significantly higher than that of the other points, compared to the original data. This indicates a better decomposition and suggests that the optimal number of components is four."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the decomposition results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands can be used to plot the first $n$ principal components.\n",
    "(Note: the `_=` part at the beginning is for webpage rendering purposes and is not strictly needed in the command) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = s.plot_decomposition_loadings(4)\n",
    "_ = s.plot_decomposition_factors(4, comp_label=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively (and usually more conveniently) we can use the following plotting method. Use the slider or the left/right arrow keys to change the index of the components in interactive plotting mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common application of PCA is noise reduction, which is achieved by dimensionality reduction. We can create a \"denoised\" version of the dataset by inverting the decomposition using only the number of principal components that we want to retain (in this case the first 4 components). This is done with the `get_decomposition_model` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = s.get_decomposition_model(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the spectra at coordinates (30,30) from the original and PCA-denoised datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(s + sc * 1j).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating and plotting the residuals at a given position can be done in one single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(s - sc).inav[30,30].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Blind source separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the previous section, the principal components are a linear mixture of EELS elemental maps and spectra, but the mixing matrix is unknown. We can use blind source separation (BSS) to estimate the mixing matrix. In this case we will use independent component analysis. BSS is performed in HyperSpy using the `blind_source_separation` method that by default uses \"FastICA\" which is a well-tested ICA routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.blind_source_separation(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are also stored in the `learning_results` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s.learning_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And can be visualised with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = s.plot_bss_loadings()\n",
    "_ = s.plot_bss_factors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or usually more conveniently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.plot_bss_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a different matrix factorization method, non-negative matrix factorization (NMF) we can decompose the data into \"elemental\" components directly. NMF replaces the orthogonality constraint in SVD by a positivity constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "s.decomposition(True, algorithm=\"NMF\", output_dimension=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = s.plot_decomposition_loadings()\n",
    "_ = s.plot_decomposition_factors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICA and NMF both do a good job, NMF being slightly better in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hyperspy]",
   "language": "python",
   "name": "conda-env-hyperspy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
